{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf3ee53-d9c0-40fb-8d14-93e374f55a2b",
   "metadata": {},
   "source": [
    "# Polars & DuckDB: DataFrames and SQL For Python Without Pandas\n",
    "--------------------------\n",
    "\n",
    "__[1. Introduction](#first-bullet)__\n",
    "\n",
    "__[2. Getting Set Up On AWS with Docker](#second-bullet)__\n",
    "\n",
    "__[3. Intro To Polars](#third-bullet)__\n",
    "\n",
    "__[4. DuckDB To The Rescue For SQL](#fourth-bullet)__\n",
    "\n",
    "__[5. Conclusions](#fifth)__\n",
    "\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "------\n",
    "\n",
    "In the last few years there has been an explosion of dataframe alternatives to [Pandas](https://pandas.pydata.org/) due to its [limitations](https://insightsndata.com/what-are-the-limitations-of-pandas-35d462990c43). Even the original author, Wes McKinney, wrote a blog post about [10 Things I Hate About Pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/). \n",
    "\n",
    "My biggest complaints about Pandas are:\n",
    "\n",
    "1. High memory usage\n",
    "2. Limited multi-core algorithms\n",
    "3. No ability to execute SQL statements (like [SparkSQL & DataFrame](https://spark.apache.org/sql/))\n",
    "4. No query planning/lazy-execution\n",
    "5. [NULL values only exist for floats not ints](https://pandas.pydata.org/docs/user_guide/integer_na.html) (this changed in Pandas 1.0+)\n",
    "6. Using [strings is inefficient](https://pandas.pydata.org/docs/user_guide/text.html) (this too changed in Pandas 1.0+\n",
    "    \n",
    "I should note that many of these issues have been addressed by the [Pandas 2.0 release](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html). And while there has been a steady march towards replacing the [NumPy](https://numpy.org/) backend with [Apache Arrow](https://arrow.apache.org/), I still feel the lack of SQL and overall API design is a major weakness of Pandas. Let me expand upon tha last point.\n",
    "\n",
    "For context I have been using a [Apache Spark](https://spark.apache.org/) since 2017 and love it not just from a performance point of view, but I also love how well the API is designed. The syntax makes sense coming from a SQL users perspective. If I want to group by a column and count in SQL or on Spark DataFrame I get what I expect either way: *A single column with the count of each item the original dataframes/tables column.* In Pandas, this is not the result.\n",
    "\n",
    "For example using this datas set from [NYC Open Data](https://opendata.cityofnewyork.us/) on [Motor Vechicle Collisions](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95), I can run a groupby-count expression on a Pandas DataFrame and I get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45782096-5f68-4198-8778-31f9badc7cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_date</th>\n",
       "      <th>crash_time</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>on_street_name</th>\n",
       "      <th>off_street_name</th>\n",
       "      <th>cross_street_name</th>\n",
       "      <th>number_of_persons_injured</th>\n",
       "      <th>...</th>\n",
       "      <th>contributing_factor_vehicle_2</th>\n",
       "      <th>contributing_factor_vehicle_3</th>\n",
       "      <th>contributing_factor_vehicle_4</th>\n",
       "      <th>contributing_factor_vehicle_5</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>vehicle_type_code1</th>\n",
       "      <th>vehicle_type_code2</th>\n",
       "      <th>vehicle_type_code_3</th>\n",
       "      <th>vehicle_type_code_4</th>\n",
       "      <th>vehicle_type_code_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borough</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRONX</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>48</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>106</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BROOKLYN</th>\n",
       "      <td>247</td>\n",
       "      <td>247</td>\n",
       "      <td>247</td>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>92</td>\n",
       "      <td>247</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>247</td>\n",
       "      <td>242</td>\n",
       "      <td>157</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MANHATTAN</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>96</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QUEENS</th>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>153</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>56</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>97</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATEN ISLAND</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               crash_date  crash_time  zip_code  latitude  longitude   \n",
       "borough                                                                \n",
       "BRONX                 107         107       107       107        107  \\\n",
       "BROOKLYN              247         247       247       245        245   \n",
       "MANHATTAN              98          98        98        96         96   \n",
       "QUEENS                154         154       153       150        150   \n",
       "STATEN ISLAND          27          27        27        26         26   \n",
       "\n",
       "               location  on_street_name  off_street_name  cross_street_name   \n",
       "borough                                                                       \n",
       "BRONX               107              59               59                 48  \\\n",
       "BROOKLYN            245             155              155                 92   \n",
       "MANHATTAN            96              52               52                 46   \n",
       "QUEENS              150              98               98                 56   \n",
       "STATEN ISLAND        26              18               18                  9   \n",
       "\n",
       "               number_of_persons_injured  ...  contributing_factor_vehicle_2   \n",
       "borough                                   ...                                  \n",
       "BRONX                                107  ...                             81  \\\n",
       "BROOKLYN                             247  ...                            192   \n",
       "MANHATTAN                             98  ...                             65   \n",
       "QUEENS                               154  ...                            120   \n",
       "STATEN ISLAND                         27  ...                             21   \n",
       "\n",
       "               contributing_factor_vehicle_3  contributing_factor_vehicle_4   \n",
       "borough                                                                       \n",
       "BRONX                                      5                              0  \\\n",
       "BROOKLYN                                  24                              7   \n",
       "MANHATTAN                                  6                              1   \n",
       "QUEENS                                     9                              2   \n",
       "STATEN ISLAND                              2                              2   \n",
       "\n",
       "               contributing_factor_vehicle_5  collision_id   \n",
       "borough                                                      \n",
       "BRONX                                      0           107  \\\n",
       "BROOKLYN                                   2           247   \n",
       "MANHATTAN                                  1            98   \n",
       "QUEENS                                     0           154   \n",
       "STATEN ISLAND                              1            27   \n",
       "\n",
       "               vehicle_type_code1  vehicle_type_code2  vehicle_type_code_3   \n",
       "borough                                                                      \n",
       "BRONX                         106                  65                    4  \\\n",
       "BROOKLYN                      242                 157                   22   \n",
       "MANHATTAN                      96                  57                    5   \n",
       "QUEENS                        154                  97                    7   \n",
       "STATEN ISLAND                  27                  19                    2   \n",
       "\n",
       "               vehicle_type_code_4  vehicle_type_code_5  \n",
       "borough                                                  \n",
       "BRONX                            0                    0  \n",
       "BROOKLYN                         7                    2  \n",
       "MANHATTAN                        1                    0  \n",
       "QUEENS                           2                    0  \n",
       "STATEN ISLAND                    2                    1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd_df = pd.read_csv(\"https://data.cityofnewyork.us/resource/h9gi-nx95.csv\")\n",
    "pd_df.groupby(\"borough\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5b421-e143-460a-9934-2dc02cdd480f",
   "metadata": {},
   "source": [
    "Notice this is the number of non nulls in every column. Not exactly what I wanted.\n",
    "\n",
    "To get what I want I have to use the syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8180247-2843-4666-9d49-1fba99d01ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "borough\n",
       "BRONX            107\n",
       "BROOKLYN         247\n",
       "MANHATTAN         98\n",
       "QUEENS           154\n",
       "STATEN ISLAND     27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.groupby(\"borough\").size() # or pd_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ce36a-269b-4cc0-a57e-827002805619",
   "metadata": {},
   "source": [
    "But this returns a [Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html). It seems like a trivial difference, but counting duplicates in a column is easy in Spark because we can use method chaining, to the do the equivalent in Pandas I have to convert the series back to a dataframe and reset the index first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d04cad-eaa3-4d84-b840-065b33ba2d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QUEENS</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STATEN ISLAND</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         borough  counts\n",
       "0          BRONX     107\n",
       "1       BROOKLYN     247\n",
       "2      MANHATTAN      98\n",
       "3         QUEENS     154\n",
       "4  STATEN ISLAND      27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.groupby(\"borough\").size().to_frame(\"counts\").reset_index().query(\"counts > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d574f-1a1f-4cdd-b3f2-0386a08c0f3f",
   "metadata": {},
   "source": [
    "Furthermore, **in Pandas there are too many ways to do the same thing.**  In my opinion, in a well designed API this shouldn't be the case. Lastly, in Pandas, window functions, which are incredibly import in SQL are just awkward to write.\n",
    "\n",
    "For years I have been using Spark for large datasets, but for smaller ones sticking with Pandas and making do. Recently though, I heard lots of hype about [Polars](https://www.pola.rs/) and [DuckDB](https://duckdb.org/) and decide to try them myself and was immediately impressed. In my opinion, Polars is not 100% mature yet, but I still  has a lot of potential, many because for me the API is much more similar to Spark's than Pandas is.\n",
    "\n",
    "In this blog post I go over my first interactions with both libraries and call out things I like and do not like, but first let's get set up to run this notebook on an AWS EC2 instance using [Docker](https://www.docker.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b09c1-e210-48b4-b6bc-a9fdedf03c94",
   "metadata": {},
   "source": [
    "## Getting Set Up On AWS with Docker <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "I have mostly used [Google Cloud](https://cloud.google.com/) for my prior personal projects, but for this project I wanted to use [Amazon Web Services](https://aws.com/). The first thing I do is create a [S3 bucket](https://aws.amazon.com/s3/). I do this from the console by signing on to [aws.com](aws.com) and going to the S3 page:\n",
    "\n",
    "![images/s3.png](images/s3.png)\n",
    "\n",
    "I can click the `Create bucket` button and create a bucket called `harmonskis` (for funskis) with all the default settings and click the`Create bucket` button on the bottom right side.\n",
    "\n",
    "Next I need to have access to read and write to and from the S3 bucket so I create an [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to do so. Going to the signin dashboard I can search for \"IAM\" and click on the link. This takes me to another site where selecting the \"Roles\" link in the the \"Access Management\" drop down on the left hand side takes me to the following:\n",
    "\n",
    "![IAM.png](images/IAM.png)\n",
    "\n",
    "I can click create the `Create role` button on the top right that takes me to the page:\n",
    "\n",
    "![create_role](images/ec2-role.png)\n",
    "\n",
    "I keep the selection of \"AWS Service\", select the \"ec2\" option and then click the `Next` button on the bottom right. This takes me to a page where I can create a policy. Searching for \"s3\" I select the following policy that gives me read/write access:\n",
    "\n",
    "![create_policy.png](images/create_policy.png)\n",
    "\n",
    "I then click the `Next` button in the bottom right which takes me to the final page:\n",
    "\n",
    "![final_role.png](images/role.png)\n",
    "\n",
    "I give the role the name \"s3acess\" (spelling isnt be best skill) and then click `Create role` in the bottom right.\n",
    "\n",
    "Next I will create my [Elastic Compute Cloud \n",
    "(EC2) Instance](https://aws.amazon.com/ec2/) instance by going to the console again and clicking on ec2, scrolling down and clicking the orange `Launch instance` button,\n",
    "\n",
    "![images/launch.png](images/launch.png)\n",
    "\n",
    "Next I have to make sure I create a `keypair` file called \"mikeskey.pem\" that I download.\n",
    "\n",
    "![images/keypair.png](images/keypair.png)\n",
    "\n",
    "Notice that in the security group I use allows SSH traffic from \"Anywhere\". Finally, under the \"Advanced details\" drop down I select \"s3acess\" (I'm living with my spelling mistake) from the \"IAM instance policy\".\n",
    "\n",
    "Once I launch the EC2 instance I can see the instance running and click on `Instance ID` as shown below:\n",
    "\n",
    "![images/instance.png](images/instance.png)\n",
    "\n",
    "I can then click on the pop up choice of `Connect`. This takes me to another page where I get the command at the bottom of the page to SSH onto my machine using the keypair I created:\n",
    "\n",
    "![images/connect.png](images/connect.png)\n",
    "\n",
    "I could ssh onto the server with the following command:\n",
    "\n",
    "    ssh -i <path-to-key>/mikeskey.pem ec2-user@<dns-address>.compute-1.amazonaws.com\n",
    "\n",
    "Note that I didnt create a user name so it defaulted to `ec2-user`. \n",
    "\n",
    "However, since I'll be running jupyter lab on a remote EC2 server I need to set up [ssh-tunneling](https://linuxize.com/post/how-to-setup-ssh-tunneling/) as described [here](https://towardsdatascience.com/setting-up-and-using-jupyter-notebooks-on-aws-61a9648db6c5) so that I can access it from the web browser on my laptop. I can do this by running the command:\n",
    "\n",
    "    ssh -i <path-to-key>/mikeskey.pem -L 8888:localhost:8888 ec2-user@<dns-address>.compute-1.amazonaws.com\n",
    "\n",
    "Next I set up git ssh-keys so I could develop on the instance as described [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent) and clone the repo. I can then set up Docker as discussed [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-container-image.html). Then I build the image and call it `polars_nb`:\n",
    "\n",
    "    sudo docker build -t polars_nb . \n",
    "\n",
    "Finally, I start up the container from this image using port forwarding and loading the current directory as the volume:\n",
    "\n",
    "    sudo docker run -ip 8888:8888 -v `pwd`:/home/jovyan/ -t polars_nb\n",
    "\n",
    "The terminal shows a link that I can copy and paste into my webbrowser, I make sure to copy the one with the 127 in it and viola it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410616c2-1553-4c04-a181-e4f9922431b2",
   "metadata": {},
   "source": [
    "## Intro To Polars <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "Now that we're set up with a notebook on an EC2 isntance we can start to discuss [Polars](https://www.pola.rs/) dataframes. The Polars library is written in Rust with Python bindings. Polars uses multi-core processing making it fast and the authors smartly used [Apache Arrow](https://arrow.apache.org/) making it efficient for cross-language in-memory dataframes as there is no serialization between the Rust and Python. According to the website the philosophy of Polars is,\n",
    "\n",
    "The goal of Polars is to provide a lightning fast DataFrame library that:\n",
    "\n",
    "* Utilizes all available cores on your machine.\n",
    "* Optimizes queries to reduce unneeded work/memory allocations.\n",
    "* Handles datasets much larger than your available RAM.\n",
    "* Has an API that is consistent and predictable.\n",
    "* Has a strict schema (data-types should be known before running the query).\n",
    "\n",
    "Let's get started! We can import polars and read in a dataset from [NY Open Data](https://opendata.cityofnewyork.us/) on [Motor Vehicle Collisions](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95) using the [read_csv](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f5f19a-7dfe-4df4-897a-637435677495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 29)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>crash_date</th><th>crash_time</th><th>borough</th><th>zip_code</th><th>latitude</th><th>longitude</th><th>location</th><th>on_street_name</th><th>off_street_name</th><th>cross_street_name</th><th>number_of_persons_injured</th><th>number_of_persons_killed</th><th>number_of_pedestrians_injured</th><th>number_of_pedestrians_killed</th><th>number_of_cyclist_injured</th><th>number_of_cyclist_killed</th><th>number_of_motorist_injured</th><th>number_of_motorist_killed</th><th>contributing_factor_vehicle_1</th><th>contributing_factor_vehicle_2</th><th>contributing_factor_vehicle_3</th><th>contributing_factor_vehicle_4</th><th>contributing_factor_vehicle_5</th><th>collision_id</th><th>vehicle_type_code1</th><th>vehicle_type_code2</th><th>vehicle_type_code_3</th><th>vehicle_type_code_4</th><th>vehicle_type_code_5</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;2021-09-11T00:…</td><td>&quot;2:39&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;WHITESTONE EXP…</td><td>&quot;20 AVENUE&quot;</td><td>null</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>&quot;Aggressive Dri…</td><td>&quot;Unspecified&quot;</td><td>null</td><td>null</td><td>null</td><td>4455765</td><td>&quot;Sedan&quot;</td><td>&quot;Sedan&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2022-03-26T00:…</td><td>&quot;11:45&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;QUEENSBORO BRI…</td><td>null</td><td>null</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>&quot;Pavement Slipp…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>4513547</td><td>&quot;Sedan&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 29)\n",
       "┌────────────┬────────────┬─────────┬──────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ crash_date ┆ crash_time ┆ borough ┆ zip_code ┆ … ┆ vehicle_t ┆ vehicle_t ┆ vehicle_t ┆ vehicle_t │\n",
       "│ ---        ┆ ---        ┆ ---     ┆ ---      ┆   ┆ ype_code2 ┆ ype_code_ ┆ ype_code_ ┆ ype_code_ │\n",
       "│ str        ┆ str        ┆ str     ┆ i64      ┆   ┆ ---       ┆ 3         ┆ 4         ┆ 5         │\n",
       "│            ┆            ┆         ┆          ┆   ┆ str       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆         ┆          ┆   ┆           ┆ str       ┆ str       ┆ str       │\n",
       "╞════════════╪════════════╪═════════╪══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2021-09-11 ┆ 2:39       ┆ null    ┆ null     ┆ … ┆ Sedan     ┆ null      ┆ null      ┆ null      │\n",
       "│ T00:00:00. ┆            ┆         ┆          ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 000        ┆            ┆         ┆          ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-03-26 ┆ 11:45      ┆ null    ┆ null     ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ T00:00:00. ┆            ┆         ┆          ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 000        ┆            ┆         ┆          ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────────┴─────────┴──────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv(\"https://data.cityofnewyork.us/resource/h9gi-nx95.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1a685-c746-496c-adf8-8000054da881",
   "metadata": {},
   "source": [
    "The initial reading of CSVs is the same as Pandas and the [head](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.head.html) dataframe method returns the top `n` rows as Pandas does. However, in addition to the printed rows, I also get shape of the dataframe as well as the datatypes of the columns. \n",
    "\n",
    "I can get the name of columns and their datatypes using the [schema](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.schema.html) method which is similar to Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafbf2d6-58ac-47d9-982d-5e9fc58aa709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crash_date': Utf8,\n",
       " 'crash_time': Utf8,\n",
       " 'borough': Utf8,\n",
       " 'zip_code': Int64,\n",
       " 'latitude': Float64,\n",
       " 'longitude': Float64,\n",
       " 'location': Utf8,\n",
       " 'on_street_name': Utf8,\n",
       " 'off_street_name': Utf8,\n",
       " 'cross_street_name': Utf8,\n",
       " 'number_of_persons_injured': Int64,\n",
       " 'number_of_persons_killed': Int64,\n",
       " 'number_of_pedestrians_injured': Int64,\n",
       " 'number_of_pedestrians_killed': Int64,\n",
       " 'number_of_cyclist_injured': Int64,\n",
       " 'number_of_cyclist_killed': Int64,\n",
       " 'number_of_motorist_injured': Int64,\n",
       " 'number_of_motorist_killed': Int64,\n",
       " 'contributing_factor_vehicle_1': Utf8,\n",
       " 'contributing_factor_vehicle_2': Utf8,\n",
       " 'contributing_factor_vehicle_3': Utf8,\n",
       " 'contributing_factor_vehicle_4': Utf8,\n",
       " 'contributing_factor_vehicle_5': Utf8,\n",
       " 'collision_id': Int64,\n",
       " 'vehicle_type_code1': Utf8,\n",
       " 'vehicle_type_code2': Utf8,\n",
       " 'vehicle_type_code_3': Utf8,\n",
       " 'vehicle_type_code_4': Utf8,\n",
       " 'vehicle_type_code_5': Utf8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e78ae-d0db-461d-a3f2-66cc25c827f9",
   "metadata": {},
   "source": [
    "We can see that the datatypes of Polars are built on top of [Arrow's datatypes](https://arrow.apache.org/docs/python/api/datatypes.html) and use Arrow arrays. This is awesome because Arrow is memory efficient and can also used for in-memory dataframes with zero-serialization across languages.\n",
    "\n",
    "The first command I tried with Polars was looking for duplicates in the dataframe. I found I could do this with the syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a16fb05-51e1-4697-a72e-e96eefd28c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>collision_id</th><th>count</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 2)\n",
       "┌──────────────┬───────┐\n",
       "│ collision_id ┆ count │\n",
       "│ ---          ┆ ---   │\n",
       "│ i64          ┆ u32   │\n",
       "╞══════════════╪═══════╡\n",
       "└──────────────┴───────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (df.groupby(\"collision_id\")\n",
    "           .count()\n",
    "           .filter(pl.col(\"count\") > 1))\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b1f2c-95ec-4aa8-9330-a259c7647716",
   "metadata": {},
   "source": [
    "Right away from the syntax I was in love.\n",
    "\n",
    "Then I saw statements returned a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7739fc-bcd7-469e-8d14-3f4dbf864bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.dataframe.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338c796-9448-4fe0-9f69-5f7fc2a42bfa",
   "metadata": {},
   "source": [
    "This is exactly what I want! I don't want a series (even though Polars does have [Series](https://pola-rs.github.io/polars/py-polars/html/reference/series/index.html) data structures). You can even print the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11fa12df-247f-4472-bc65-45a74469a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (0, 2)\n",
      "┌──────────────┬───────┐\n",
      "│ collision_id ┆ count │\n",
      "│ ---          ┆ ---   │\n",
      "│ i64          ┆ u32   │\n",
      "╞══════════════╪═══════╡\n",
      "└──────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96696e39-9631-4a80-b915-e9757e730818",
   "metadata": {},
   "source": [
    "This turns out to be helpful when you have lazy execution (which I'll go over later). The next thing I tried was to access the column of the dataframe by using the dot operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c46a46-da02-4293-b10c-ca233b6f71a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'crash_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrash_date\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'crash_date'"
     ]
    }
   ],
   "source": [
    "df.crash_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c4941-d1ec-4e65-8a8a-4810f61ef297",
   "metadata": {},
   "source": [
    "I was actually happy to see this was not implemented! For me a column in a dataframe should not be accessed this way. The dot operator is meant to access attributes of the class.\n",
    "\n",
    "Instead we can access the column of the dataframe like a dictionary's key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437add7e-f16c-46f4-b6df-f1c98e00ab96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"crash_date\"].is_null().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe9004-55bf-4405-a04c-4a36c7b3051f",
   "metadata": {},
   "source": [
    "The crash dates are strings that I wanted to convert to datetime type (I'm doing this to build up to more complex queries). I can see the format of the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1690a44-9e14-4164-92ae-6d88010b80be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-09-11T00:00:00.000'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['crash_date'][0] # the .loc method doesnt exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a087e12-9cc1-44ee-afb8-865769770ee7",
   "metadata": {},
   "source": [
    "To do so, I write two queries:\n",
    "\n",
    "1. The first query extracts the year-month-day and writes it as a string in the format YYYY-MM-DD\n",
    "2. The second query converts the YYYY-MM-DD strings into timestamp objects\n",
    "\n",
    "For the first query I can extract the year-month-day from the string and assign that to a new column named `crash_date_str`. Note the syntax to create a new column in Polars is [with_columns](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.with_columns.html) (similar to [withColumn](https://sparkbyexamples.com/pyspark/pyspark-withcolumn/) in Spark) and I have to use the [col](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.col.html) function similar to Spark! I can get the first 10 characters of the string using the vectorized [str method](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html) similar to Pandas. Finally, I rename the new column `crash_data_str` using the [alias](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.Expr.alias.html) function (again just like Spark). The default for the `with_column` is to label the new column name the same as the old column name, so we use alias to rename it. \n",
    "\n",
    "In the second query I use the vectorized string method [strptime](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.Expr.str.strptime.html) to convert the `crash_date_str` column to a PyArrow datetime object and rename that column `crash_date` (overriding the old column with this name). \n",
    "\n",
    "These two queries are chained together and the results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f3b0ae-9f70-4504-a74b-f2823ac1bd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>crash_date</th><th>crash_date_str</th></tr><tr><td>datetime[μs]</td><td>str</td></tr></thead><tbody><tr><td>2021-09-11 00:00:00</td><td>&quot;2021-09-11&quot;</td></tr><tr><td>2022-03-26 00:00:00</td><td>&quot;2022-03-26&quot;</td></tr><tr><td>2022-06-29 00:00:00</td><td>&quot;2022-06-29&quot;</td></tr><tr><td>2021-09-11 00:00:00</td><td>&quot;2021-09-11&quot;</td></tr><tr><td>2021-12-14 00:00:00</td><td>&quot;2021-12-14&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────┬────────────────┐\n",
       "│ crash_date          ┆ crash_date_str │\n",
       "│ ---                 ┆ ---            │\n",
       "│ datetime[μs]        ┆ str            │\n",
       "╞═════════════════════╪════════════════╡\n",
       "│ 2021-09-11 00:00:00 ┆ 2021-09-11     │\n",
       "│ 2022-03-26 00:00:00 ┆ 2022-03-26     │\n",
       "│ 2022-06-29 00:00:00 ┆ 2022-06-29     │\n",
       "│ 2021-09-11 00:00:00 ┆ 2021-09-11     │\n",
       "│ 2021-12-14 00:00:00 ┆ 2021-12-14     │\n",
       "└─────────────────────┴────────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.with_columns(\n",
    "            pl.col(\"crash_date\").str.slice(0, length=10).alias(\"crash_date_str\")\n",
    "      ).with_columns(\n",
    "            pl.col(\"crash_date_str\").str.strptime(\n",
    "                pl.Datetime, \"%Y-%m-%d\", strict=False).alias(\"crash_date\")\n",
    ")\n",
    "\n",
    "df.select([\"crash_date\", \"crash_date_str\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08663530-c746-4e42-860f-3453da3b9646",
   "metadata": {},
   "source": [
    "Notice the [col](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.col.html) function in Polars lets me access derived columns that are not in the original dataframe. In Pandas to do the same operations I would have to use a lambda function within an assign function:\n",
    "\n",
    "    df.assign(crash_date=lambda: df[\"crash_date_str\"].str.strptime(...))\n",
    "\n",
    "I can see the number of crashes in each borough of NYC with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b045c9-1ec2-4ebd-86fc-7fd13d700e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (6, 2)\n",
      "┌───────────────┬───────┐\n",
      "│ borough       ┆ count │\n",
      "│ ---           ┆ ---   │\n",
      "│ str           ┆ u32   │\n",
      "╞═══════════════╪═══════╡\n",
      "│ MANHATTAN     ┆ 98    │\n",
      "│ STATEN ISLAND ┆ 27    │\n",
      "│ BROOKLYN      ┆ 247   │\n",
      "│ BRONX         ┆ 107   │\n",
      "│ null          ┆ 367   │\n",
      "│ QUEENS        ┆ 154   │\n",
      "└───────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(\"borough\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1aa9b1-e3d6-484e-90db-8742d5cd5fdb",
   "metadata": {},
   "source": [
    "There is a borough value of NULL. I can filter this out with the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1682237a-b41e-4042-95e5-a4a5922b9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_df = df.filter(pl.col(\"borough\").is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f984a-b86b-4f63-98a0-cde2fe62b4cf",
   "metadata": {},
   "source": [
    "Now I can get just the unique values of non-null boroughs with the query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2dd6e6-8e72-430c-8160-cedbae02ca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 1)\n",
      "┌───────────────┐\n",
      "│ borough       │\n",
      "│ ---           │\n",
      "│ str           │\n",
      "╞═══════════════╡\n",
      "│ STATEN ISLAND │\n",
      "│ MANHATTAN     │\n",
      "│ QUEENS        │\n",
      "│ BRONX         │\n",
      "│ BROOKLYN      │\n",
      "└───────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(df.filter(pl.col(\"borough\").is_not_null())\n",
    "        .select(\"borough\")\n",
    "        .unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb4cc8-734d-4f79-afa6-e0626be6cc59",
   "metadata": {},
   "source": [
    "Notice that I can use the [select](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.select.html) method in Polars to select just the columns I need. This is actually pretty powerful, as I can select columns and run queries on them similar to [selectEpr](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html) in Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cdde422-3f47-4c09-abf4-9ddfcb790313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌───────────┬─────────────────────────────────┐\n",
      "│ borough   ┆ number_of_persons_injured_plus1 │\n",
      "│ ---       ┆ ---                             │\n",
      "│ str       ┆ i64                             │\n",
      "╞═══════════╪═════════════════════════════════╡\n",
      "│ BROOKLYN  ┆ 1                               │\n",
      "│ BROOKLYN  ┆ 1                               │\n",
      "│ BRONX     ┆ 3                               │\n",
      "│ BROOKLYN  ┆ 1                               │\n",
      "│ MANHATTAN ┆ 1                               │\n",
      "└───────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    " df.filter(pl.col(\"borough\").is_not_null())\n",
    "   .select([\n",
    "       \"borough\", \n",
    "       (pl.col(\"number_of_persons_injured\")  + 1).alias(\"number_of_persons_injured_plus1\")\n",
    "    ]).head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd93c8-8926-4803-9d81-220d104bfa63",
   "metadata": {},
   "source": [
    "Doing the same query in Pandas is not as elegant or readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "180a8b21-524a-400b-8e4b-83bb7f1c030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>number_of_persons_injured_plus1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     borough  number_of_persons_injured_plus1\n",
       "3   BROOKLYN                                1\n",
       "4   BROOKLYN                                1\n",
       "7      BRONX                                3\n",
       "8   BROOKLYN                                1\n",
       "9  MANHATTAN                                1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd_df[~pd_df[\"borough\"].isnull()]\n",
    "      .assign(number_of_persons_injured_plus1=pd_df[\"number_of_persons_injured\"] + 1)\n",
    "      [[\"borough\", \"number_of_persons_injured_plus1\"]]\n",
    "      .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54d249-1291-4287-a10b-be714e28cf92",
   "metadata": {},
   "source": [
    "To me, the Polars query is so much easier to read. And what's more is that it's actually more efficient. The Pandas dataframe transforms the whole dataset, then subsets the columns to return just two. On the other hand Polars subsets the two columns first and then transforms just those two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f1e0c-7e90-49ea-b427-39890f3d5f4b",
   "metadata": {},
   "source": [
    "Now I can create a Polars dataframe the exact same way as in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf1932d-95f7-4efc-8681-acafb3d2e1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌───────────────┬────────────┬───────┐\n",
      "│ borough       ┆ population ┆ area  │\n",
      "│ ---           ┆ ---        ┆ ---   │\n",
      "│ str           ┆ i64        ┆ f64   │\n",
      "╞═══════════════╪════════════╪═══════╡\n",
      "│ BROOKLYN      ┆ 2590516    ┆ 179.7 │\n",
      "│ BRONX         ┆ 1379946    ┆ 109.2 │\n",
      "│ MANHATTAN     ┆ 1596273    ┆ 58.68 │\n",
      "│ STATEN ISLAND ┆ 2278029    ┆ 281.6 │\n",
      "│ QUEENS        ┆ 378977     ┆ 149.0 │\n",
      "└───────────────┴────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "borough_df = pl.DataFrame({\n",
    "                \"borough\": [\"BROOKLYN\", \"BRONX\", \"MANHATTAN\", \"STATEN ISLAND\", \"QUEENS\"],\n",
    "                \"population\": [2590516, 1379946, 1596273, 2278029, 378977],\n",
    "                \"area\":[179.7, 109.2, 58.68, 281.6, 149.0]\n",
    "})\n",
    "\n",
    "print(borough_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f30190-9190-4748-8ab3-33e7c06acb3d",
   "metadata": {},
   "source": [
    "This is the population and area of the boroughs which I got from Wikipedia. I'll save it to s3. It was a little awkward to write to s3 with Polars directly so I'll first convert the dataframe to Pandas and then write to s3:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fee1c3-a1ac-4190-b7a2-e2f2d7c79d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_df.to_pandas().to_parquet(\"s3://harmonskis/nyc_populations.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a75fc-7c45-4c89-868b-1abdf1860187",
   "metadata": {},
   "source": [
    "However, reading from s3 is just the same as with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d5edbee-3e2e-45c2-85f4-bd3736a3beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_df = pl.read_parquet(\"s3://harmonskis/nyc_populations.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9204a05-3ae4-44ed-b5eb-05d1533c1372",
   "metadata": {},
   "source": [
    "We'll use it to go over a more complicated query:\n",
    "\n",
    "    Get the total number of injuries per borough then join that result to the borough dataframe to get the injuries by population and finally sort them by borough name.\n",
    "\n",
    "In Polars this can be using method chaining on the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06db93ed-9664-46aa-bae2-c6d05e0aeff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌───────────────┬─────────────────────────┐\n",
      "│ borough       ┆ injuries_per_population │\n",
      "│ ---           ┆ ---                     │\n",
      "│ str           ┆ f64                     │\n",
      "╞═══════════════╪═════════════════════════╡\n",
      "│ BRONX         ┆ 0.000033                │\n",
      "│ BROOKLYN      ┆ 0.000045                │\n",
      "│ MANHATTAN     ┆ 0.000025                │\n",
      "│ QUEENS        ┆ 0.000193                │\n",
      "│ STATEN ISLAND ┆ 0.000007                │\n",
      "└───────────────┴─────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    " df.filter(pl.col(\"borough\").is_not_null())\n",
    "   .select([\"borough\", \"number_of_persons_injured\"])\n",
    "   .groupby(\"borough\")\n",
    "   .sum()\n",
    "   .join(borough_df, on=[\"borough\"])\n",
    "   .select([\n",
    "       \"borough\", \n",
    "       (pl.col(\"number_of_persons_injured\") / pl.col(\"population\")).alias(\"injuries_per_population\")\n",
    "   ])\n",
    "   .sort(pl.col(\"borough\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99dd45-37d7-48bd-a7f4-5e6a8c03fb5f",
   "metadata": {},
   "source": [
    "Doing the same query in the Pandas API would be an awkward mess. As we can see in Polars it's very easy to use method chaining and the resulting syntax reads pretty similar to SQL! \n",
    "\n",
    "Which brings me to something that was super exciting to see in Polars: [sqlcontext](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.SQLContext.execute.html). SQLContext in Polars can be used to create a table from a Polars dataframe and then run SQL commands that return another Polars dataframe. \n",
    "\n",
    "We can see this by creating a table called `crashes` from the dataframe `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57aa37a5-cff8-405f-a1be-b80c52cd73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = pl.SQLContext(crashes=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33954ddd-8562-4c4e-a6cc-a1b5c4fa2ba3",
   "metadata": {},
   "source": [
    "Now I can get the sum of every crash per day in each borough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "253abd24-8ba7-4caf-9242-cbf192da5890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────┬─────────────────────┬───────────────────────────┐\n",
      "│ borough ┆ day                 ┆ number_of_persons_injured │\n",
      "│ ---     ┆ ---                 ┆ ---                       │\n",
      "│ str     ┆ datetime[μs]        ┆ i64                       │\n",
      "╞═════════╪═════════════════════╪═══════════════════════════╡\n",
      "│ BRONX   ┆ 2021-02-26 00:00:00 ┆ 0                         │\n",
      "│ BRONX   ┆ 2021-04-06 00:00:00 ┆ 0                         │\n",
      "│ BRONX   ┆ 2021-04-08 00:00:00 ┆ 0                         │\n",
      "│ BRONX   ┆ 2021-04-10 00:00:00 ┆ 4                         │\n",
      "│ BRONX   ┆ 2021-04-11 00:00:00 ┆ 0                         │\n",
      "└─────────┴─────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "daily_df = ctx.execute(\"\"\"\n",
    "    SELECT\n",
    "        borough,\n",
    "        crash_date AS day,\n",
    "        SUM(number_of_persons_injured)\n",
    "    FROM \n",
    "        crashes\n",
    "    WHERE \n",
    "        borough IS NOT NULL\n",
    "    GROUP BY \n",
    "        borough, crash_date\n",
    "    ORDER BY \n",
    "        borough, day\n",
    "\"\"\")\n",
    "\n",
    "print(daily_df.collect().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde53df-16a9-4ce6-a6c9-52552b2b88fa",
   "metadata": {},
   "source": [
    "Notice I had to use `collect()` function to get the results. That is because by default SQL in Polars uses lazy execution.\n",
    "\n",
    "You can see evidence of this when printing the resulting dataframe; it actually prints the query plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6709aecb-5ea3-4042-b02f-35a76f89ba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n",
      "\n",
      "SORT BY [col(\"borough\"), col(\"day\")]\n",
      "   SELECT [col(\"borough\"), col(\"crash_date\").alias(\"day\"), col(\"number_of_persons_injured\")] FROM\n",
      "    AGGREGATE\n",
      "    \t[col(\"number_of_persons_injured\").sum()] BY [col(\"borough\"), col(\"crash_date\")] FROM\n",
      "      FILTER col(\"borough\").is_not_null() FROM\n",
      "      DF [\"crash_date\", \"crash_time\", \"borough\", \"zip_code\"]; PROJECT */30 COLUMNS; SELECTION: \"None\"\n"
     ]
    }
   ],
   "source": [
    "print(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44e052-3855-406f-b7f8-139708c0c665",
   "metadata": {},
   "source": [
    "To get back a Polars dataframe from this result I would have to use the `eager=True` parameter in the execute method.\n",
    "\n",
    "I can register this new dataframe as a table called `daily_crashes` in the SQLContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fab83d06-5f21-4ac1-81ad-0a09cd6a3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ctx.register(\"daily_crashes\", daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbe6a5-442e-4dff-b58a-1165c4c03eb8",
   "metadata": {},
   "source": [
    "I can see the tables that are registered using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae2c969-1e49-4dae-a409-486f513885c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crashes', 'daily_crashes']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571026f5-266f-4516-a0a0-829fa068a9be",
   "metadata": {},
   "source": [
    "Now say I want to get the current day's number of injured people and the prior days; I could use the [lag](https://www.sqlshack.com/sql-lag-function-overview-and-examples/) function in SQL to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "461358da-6a28-4799-a722-d2ade734ea5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidOperationError",
     "evalue": "unsupported SQL function: lag",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidOperationError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m    SELECT\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m        borough,\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        day,\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m        number_of_persons_injured,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m        LAG(1,number_of_persons_injured) \u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m            OVER (\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m            PARTITION BY borough \u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m            ORDER BY day ASC\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m            ) AS prior_day_injured\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43mFROM\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m    daily_crashes\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43mORDER BY \u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m    borough,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m    day DESC\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/polars/sql/context.py:282\u001b[0m, in \u001b[0;36mSQLContext.execute\u001b[0;34m(self, query, eager)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, eager: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LazyFrame \u001b[38;5;241m|\u001b[39m DataFrame:\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    Parse the given SQL query and execute it against the registered frame data.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    └────────┴─────────────┴─────────┘\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     res \u001b[38;5;241m=\u001b[39m wrap_ldf(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctxt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcollect() \u001b[38;5;28;01mif\u001b[39;00m (eager \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eager_execution) \u001b[38;5;28;01melse\u001b[39;00m res\n",
      "\u001b[0;31mInvalidOperationError\u001b[0m: unsupported SQL function: lag"
     ]
    }
   ],
   "source": [
    "ctx.execute(\"\"\"\n",
    "    SELECT\n",
    "        borough,\n",
    "        day,\n",
    "        number_of_persons_injured,\n",
    "        LAG(1,number_of_persons_injured) \n",
    "            OVER (\n",
    "            PARTITION BY borough \n",
    "            ORDER BY day ASC\n",
    "            ) AS prior_day_injured\n",
    "FROM\n",
    "    daily_crashes\n",
    "ORDER BY \n",
    "    borough,\n",
    "    day DESC\n",
    "\"\"\", eager=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a484e-485c-4c88-ae35-05979b36542e",
   "metadata": {},
   "source": [
    "I finally hit snag in Polars: their doesnt seem to be a lot of support for Window functions. This was initially disappointing since the library was so promising!\n",
    "\n",
    "Upon further research I found window functions are supported, infact they are [**VERY WELL supported!**](https://pola-rs.github.io/polars-book/user-guide/expressions/window/). The query I was trying to turns out to be fairly easy to write as dataframe operations using the [over](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.Expr.over.html) expression. This is exactly the same as SQL where the column names within the `over(...)` operator are the columns you partition by. You can the sort within each partition (or group as they say in Polars) and use shift instead of LAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffa3a189-119d-4cab-93fb-f7dd27b3d923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 4)\n",
      "┌─────────┬─────────────────────┬───────────────────────────┬───────────────────┐\n",
      "│ borough ┆ day                 ┆ number_of_persons_injured ┆ prior_day_injured │\n",
      "│ ---     ┆ ---                 ┆ ---                       ┆ ---               │\n",
      "│ str     ┆ datetime[μs]        ┆ i64                       ┆ i64               │\n",
      "╞═════════╪═════════════════════╪═══════════════════════════╪═══════════════════╡\n",
      "│ BRONX   ┆ 2021-02-26 00:00:00 ┆ 0                         ┆ null              │\n",
      "│ BRONX   ┆ 2021-04-06 00:00:00 ┆ 0                         ┆ 0                 │\n",
      "│ BRONX   ┆ 2021-04-08 00:00:00 ┆ 0                         ┆ 0                 │\n",
      "│ BRONX   ┆ 2021-04-10 00:00:00 ┆ 4                         ┆ 0                 │\n",
      "│ BRONX   ┆ 2021-04-11 00:00:00 ┆ 0                         ┆ 4                 │\n",
      "│ BRONX   ┆ 2021-04-12 00:00:00 ┆ 0                         ┆ 0                 │\n",
      "│ BRONX   ┆ 2021-04-13 00:00:00 ┆ 3                         ┆ 0                 │\n",
      "│ BRONX   ┆ 2021-04-14 00:00:00 ┆ 3                         ┆ 3                 │\n",
      "└─────────┴─────────────────────┴───────────────────────────┴───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    daily_df.with_columns(\n",
    "            pl.col(\"number_of_persons_injured\")\n",
    "              .sort_by(\"day\", descending=False)\n",
    "              .shift(periods=1)\n",
    "              .over(\"borough\")\n",
    "              .alias(\"prior_day_injured\")\n",
    ").collect().head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82b1ac-3a54-468d-9865-0e65106f2d5c",
   "metadata": {},
   "source": [
    "It turns out you can do the same thing with Pandas as shown below.\n",
    "\n",
    "Note that I have to collect the lazy datafame and convert it to Pandas first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5975d5a-0cb7-4d60-b5ea-c7710015c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_daily_df = daily_df.collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e7f2054-6ce6-467e-ab69-abff492424dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>day</th>\n",
       "      <th>number_of_persons_injured</th>\n",
       "      <th>prior_day_injured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRONX</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  borough        day  number_of_persons_injured  prior_day_injured\n",
       "0   BRONX 2021-02-26                          0                NaN\n",
       "1   BRONX 2021-04-06                          0                0.0\n",
       "2   BRONX 2021-04-08                          0                0.0\n",
       "3   BRONX 2021-04-10                          4                0.0\n",
       "4   BRONX 2021-04-11                          0                4.0\n",
       "5   BRONX 2021-04-12                          0                0.0\n",
       "6   BRONX 2021-04-13                          3                0.0\n",
       "7   BRONX 2021-04-14                          3                3.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_daily_df = pd_daily_df.assign(prior_day_injured=\n",
    "                        pd_daily_df.sort_values(by=['day'], ascending=True)\n",
    "                          .groupby(['borough'])\n",
    "                          ['number_of_persons_injured']\n",
    "                          .shift(1))\n",
    "\n",
    "pd_daily_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857fcd8-c8ef-463f-b85d-cd9726385a33",
   "metadata": {},
   "source": [
    "Syntactically, I still perfer the Polars to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26ab2a-6035-44fb-b3a2-0563ee5b491c",
   "metadata": {},
   "source": [
    "But let's I really want to use SQL and not do things in the dataframe, atleast to me, it doesnt seem possible with Polars.\n",
    "\n",
    "Luckily there is another library that support blazingly fast SQL queries and integrates with Polars (and Pandas) directly: DuckDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5307c3-e2f6-4b87-a140-5aa1a50a9878",
   "metadata": {},
   "source": [
    "## DuckDB To The Rescue For SQL <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d84eaf-cbcc-4564-a7e7-3761d0d99a43",
   "metadata": {},
   "source": [
    "I heard about [DuckDB](https://duckdb.org/) when I saw someone star it on github and thought it was \"Yet Another SQL Engine\". While DuckDB is a SQL engine, it does much more than I thought a SQL engine could! \n",
    "\n",
    "DuckDB is a parallel query processing library written in C++ and according to their website:\n",
    "\n",
    "        DuckDB is designed to support analytical query workloads, also known as Online analytical processing (OLAP). These workloads are characterized by complex, relatively long-running queries that process significant portions of the stored dataset, for example aggregations over entire tables or joins between several large tables.\n",
    "        ...\n",
    "        DuckDB contains a columnar-vectorized query execution engine, where queries are still interpreted, but a large batch of values (a “vector”) are processed in one operation.\n",
    "\n",
    "In other words, DuckDB can be used for fast SQL query execution on large datasets. For example the above query that failed in Polars runs perfectly using DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b72d3d8-f580-40b9-8922-1252f1414582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "query = duckdb.sql(\"\"\"\n",
    "    SELECT\n",
    "        borough,\n",
    "        day,\n",
    "        number_of_persons_injured,\n",
    "        LAG(1, number_of_persons_injured) \n",
    "            OVER (\n",
    "                PARTITION BY borough \n",
    "                ORDER BY day ASC\n",
    "                ) as prior_day_injured\n",
    "FROM\n",
    "    daily_df\n",
    "ORDER BY \n",
    "    borough,\n",
    "    day DESC\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e0b63-9a3f-4db3-b013-d2184776fa6d",
   "metadata": {},
   "source": [
    "Now we can see the output of the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "626b0f64-8589-4607-ac43-43008ecdf6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬─────────────────────┬───────────────────────────┬───────────────────┐\n",
       "│ borough │         day         │ number_of_persons_injured │ prior_day_injured │\n",
       "│ varchar │      timestamp      │           int64           │       int32       │\n",
       "├─────────┼─────────────────────┼───────────────────────────┼───────────────────┤\n",
       "│ BRONX   │ 2022-04-24 00:00:00 │                         0 │                 1 │\n",
       "│ BRONX   │ 2022-03-26 00:00:00 │                         7 │                 1 │\n",
       "│ BRONX   │ 2022-03-25 00:00:00 │                         1 │                 1 │\n",
       "│ BRONX   │ 2022-03-24 00:00:00 │                         1 │                 1 │\n",
       "│ BRONX   │ 2022-03-22 00:00:00 │                         1 │                 1 │\n",
       "└─────────┴─────────────────────┴───────────────────────────┴───────────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7284e-1d7e-4538-b2de-40b222498820",
   "metadata": {},
   "source": [
    "We can return the result as polars dataframe using the `pl` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d2e41c8-65b9-4e3e-bb5b-c4b6cb902a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 4)\n",
      "┌─────────┬─────────────────────┬───────────────────────────┬───────────────────┐\n",
      "│ borough ┆ day                 ┆ number_of_persons_injured ┆ prior_day_injured │\n",
      "│ ---     ┆ ---                 ┆ ---                       ┆ ---               │\n",
      "│ str     ┆ datetime[μs]        ┆ i64                       ┆ i32               │\n",
      "╞═════════╪═════════════════════╪═══════════════════════════╪═══════════════════╡\n",
      "│ BRONX   ┆ 2022-04-24 00:00:00 ┆ 0                         ┆ 1                 │\n",
      "│ BRONX   ┆ 2022-03-26 00:00:00 ┆ 7                         ┆ 1                 │\n",
      "│ BRONX   ┆ 2022-03-25 00:00:00 ┆ 1                         ┆ 1                 │\n",
      "│ BRONX   ┆ 2022-03-24 00:00:00 ┆ 1                         ┆ 1                 │\n",
      "│ BRONX   ┆ 2022-03-22 00:00:00 ┆ 1                         ┆ 1                 │\n",
      "└─────────┴─────────────────────┴───────────────────────────┴───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "day_prior_df = query.pl()\n",
    "print(day_prior_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5c1c9-0a5e-486b-aaa6-cd082ad331f3",
   "metadata": {},
   "source": [
    "Now we can see another cool part of DuckDB, you can execute SQL directly on local files!\n",
    "\n",
    "First we save the daily crash dataframe as [Parquet](https://parquet.apache.org/)  file, but first remember it's a \"lazy dataframe\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd8deafd-a6b6-4047-ab55-5dba4039794a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>naive plan: (run <b>LazyFrame.explain(optimized=True)</b> to see the optimized plan)</i>\n",
       "    <p></p>\n",
       "    <div>SORT BY [col(\"borough\"), col(\"day\")]<p></p>   SELECT [col(\"borough\"), col(\"crash_date\").alias(\"day\"), col(\"number_of_persons_injured\")] FROM<p></p>    AGGREGATE<p></p>    \t[col(\"number_of_persons_injured\").sum()] BY [col(\"borough\"), col(\"crash_date\")] FROM<p></p>      FILTER col(\"borough\").is_not_null() FROM<p></p>      DF [\"crash_date\", \"crash_time\", \"borough\", \"zip_code\"]; PROJECT */30 COLUMNS; SELECTION: \"None\"</div>"
      ],
      "text/plain": [
       "<LazyFrame [3 cols, {\"borough\": Utf8 … \"number_of_persons_injured\": Int64}] at 0xFFFF5FEB4AC0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7196fb2-9268-489b-910e-0dc1c6408cb0",
   "metadata": {},
   "source": [
    "It turns out you cant write lazy dataframes as Parquet using Polars. So first we'll collect it and then write it to parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c5309bf-1365-4825-afa5-061c543cdba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.collect().write_parquet(\"daily_crashes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1daeae5-cd40-4c7c-aaaf-d373da92e832",
   "metadata": {},
   "source": [
    "[Apache Parquet](https://parquet.apache.org/) is a compressed columnar-stored file format that is great for analytical queries. Column-based formats are particularly good for [OLAP](https://aws.amazon.com/what-is/olap/) queries since columns can subsetted and be read in continuously allowing for aggregations to be easily performed on them. The datatypes for each column in Parquet are known which allows the format to be compressed. Since the columns and datatypes are known metadata we can read them in with the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49159545-2a8f-463e-9242-34caeb906e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>file_name</th><th>name</th><th>type</th><th>type_length</th><th>repetition_type</th><th>num_children</th><th>converted_type</th><th>scale</th><th>precision</th><th>field_id</th><th>logical_type</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;daily_crashes.…</td><td>&quot;root&quot;</td><td>null</td><td>null</td><td>null</td><td>3</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;daily_crashes.…</td><td>&quot;borough&quot;</td><td>&quot;BYTE_ARRAY&quot;</td><td>null</td><td>&quot;OPTIONAL&quot;</td><td>null</td><td>&quot;UTF8&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;StringType()&quot;</td></tr><tr><td>&quot;daily_crashes.…</td><td>&quot;day&quot;</td><td>&quot;INT64&quot;</td><td>null</td><td>&quot;OPTIONAL&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;TimestampType(…</td></tr><tr><td>&quot;daily_crashes.…</td><td>&quot;number_of_pers…</td><td>&quot;INT64&quot;</td><td>null</td><td>&quot;OPTIONAL&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 11)\n",
       "┌────────────┬────────────┬────────────┬────────────┬───┬───────┬───────────┬──────────┬───────────┐\n",
       "│ file_name  ┆ name       ┆ type       ┆ type_lengt ┆ … ┆ scale ┆ precision ┆ field_id ┆ logical_t │\n",
       "│ ---        ┆ ---        ┆ ---        ┆ h          ┆   ┆ ---   ┆ ---       ┆ ---      ┆ ype       │\n",
       "│ str        ┆ str        ┆ str        ┆ ---        ┆   ┆ i64   ┆ i64       ┆ i64      ┆ ---       │\n",
       "│            ┆            ┆            ┆ str        ┆   ┆       ┆           ┆          ┆ str       │\n",
       "╞════════════╪════════════╪════════════╪════════════╪═══╪═══════╪═══════════╪══════════╪═══════════╡\n",
       "│ daily_cras ┆ root       ┆ null       ┆ null       ┆ … ┆ null  ┆ null      ┆ null     ┆ null      │\n",
       "│ hes.parque ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆           │\n",
       "│ t          ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆           │\n",
       "│ daily_cras ┆ borough    ┆ BYTE_ARRAY ┆ null       ┆ … ┆ null  ┆ null      ┆ null     ┆ StringTyp │\n",
       "│ hes.parque ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆ e()       │\n",
       "│ t          ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆           │\n",
       "│ daily_cras ┆ day        ┆ INT64      ┆ null       ┆ … ┆ null  ┆ null      ┆ null     ┆ Timestamp │\n",
       "│ hes.parque ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆ Type(isAd │\n",
       "│ t          ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆ justedToU │\n",
       "│            ┆            ┆            ┆            ┆   ┆       ┆           ┆          ┆ TC=0,…    │\n",
       "│ daily_cras ┆ number_of_ ┆ INT64      ┆ null       ┆ … ┆ null  ┆ null      ┆ null     ┆ null      │\n",
       "│ hes.parque ┆ persons_in ┆            ┆            ┆   ┆       ┆           ┆          ┆           │\n",
       "│ t          ┆ jured      ┆            ┆            ┆   ┆       ┆           ┆          ┆           │\n",
       "└────────────┴────────────┴────────────┴────────────┴───┴───────┴───────────┴──────────┴───────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"SELECT * FROM parquet_schema(daily_crashes.parquet)\").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11776a-9999-4cba-81ab-1ad47a03262f",
   "metadata": {},
   "source": [
    "Now we can perform queries on the actualy files without having to resort to dataframes at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c016a097-4ca5-42d7-a36c-e13c1efcb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = duckdb.sql(\"\"\"\n",
    "    SELECT\n",
    "        borough,\n",
    "        day,\n",
    "        number_of_persons_injured,\n",
    "        SUM(number_of_persons_injured) \n",
    "            OVER (\n",
    "                PARTITION BY borough \n",
    "                ORDER BY day ASC\n",
    "                ) AS cumulative_injuried\n",
    "    FROM \n",
    "        read_parquet(daily_crashes.parquet)\n",
    "    ORDER BY\n",
    "        borough,\n",
    "        day ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39cc91f-81a2-431e-b3be-5a2bf26b1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 4)\n",
      "┌─────────┬─────────────────────────┬───────────────────────────┬─────────────────────┐\n",
      "│ borough ┆ day                     ┆ number_of_persons_injured ┆ cumulative_injuried │\n",
      "│ ---     ┆ ---                     ┆ ---                       ┆ ---                 │\n",
      "│ str     ┆ str                     ┆ i64                       ┆ f64                 │\n",
      "╞═════════╪═════════════════════════╪═══════════════════════════╪═════════════════════╡\n",
      "│ BRONX   ┆ 2021-02-26T00:00:00.000 ┆ 0                         ┆ 0.0                 │\n",
      "│ BRONX   ┆ 2021-04-06T00:00:00.000 ┆ 0                         ┆ 0.0                 │\n",
      "│ BRONX   ┆ 2021-04-08T00:00:00.000 ┆ 0                         ┆ 0.0                 │\n",
      "│ BRONX   ┆ 2021-04-10T00:00:00.000 ┆ 4                         ┆ 4.0                 │\n",
      "│ BRONX   ┆ 2021-04-11T00:00:00.000 ┆ 0                         ┆ 4.0                 │\n",
      "│ BRONX   ┆ 2021-04-12T00:00:00.000 ┆ 0                         ┆ 4.0                 │\n",
      "│ BRONX   ┆ 2021-04-13T00:00:00.000 ┆ 3                         ┆ 7.0                 │\n",
      "│ BRONX   ┆ 2021-04-14T00:00:00.000 ┆ 3                         ┆ 10.0                │\n",
      "└─────────┴─────────────────────────┴───────────────────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(query.pl().head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e247d5-ace6-4c90-927e-97e9090a864b",
   "metadata": {},
   "source": [
    "Pretty cool!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60192db-4916-4c22-b3bd-f8322c41ca9e",
   "metadata": {},
   "source": [
    "## Conclusions <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edceb7-1d3d-4975-820f-67087937ef31",
   "metadata": {},
   "source": [
    "In this post I quickly covered what I view as the limitations of Pandas library. Next I covered how to get set up in with \n",
    "Jupyter lab using [Docker](https://www.docker.com/) on [AWS](https://aws.amazon.com/) and covered some basics of [Polars](https://www.pola.rs/), [DuckDB](https://duckdb.org/) and how to use the two in combination. The benefits of Polars is that,\n",
    "\n",
    "* It allows for fast parallel querying on dataframes.\n",
    "* It uses Apache Arrow for backend datatypes making it memory efficient.\n",
    "* It has both lazy and eager execution mode.\n",
    "* It allows for SQL queries directly on dataframes.\n",
    "* Its API is similar to Spark's API and allows for highly readable queries using method chaining.\n",
    "\n",
    "I am still new to both libraries, but looking forward to learning more about them.\n",
    "\n",
    "Hope you enjoyed reading this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
